var tipuesearch = {"pages":[{"title":"Should we say stop to the syntactical growth of Python?","text":"The language has been literally 'evolving' since the version that many of us have begun (maybe since the initial day, if you are Guido). I'm not talking about any features in specific, but rather the whole language. If you compare a code fragment that you wrote 5 years ago, with the 'refactored' version that you would write if it were today the difference is obvious. We've seen a lot of new syntax popping into our lives, in just the last 5 years (starting from 3.5): PEP 448 >>> * range ( 4 ), 4 ( 0 , 1 , 2 , 3 , 4 ) >>> [ * range ( 4 ), 4 ] [ 0 , 1 , 2 , 3 , 4 ] >>> { * range ( 4 ), 4 } { 0 , 1 , 2 , 3 , 4 } >>> { 'x' : 1 , ** { 'y' : 2 }} { 'x' : 1 , 'y' : 2 } PEP 465 , a.k.a matrix operator S = ( H @ beta - r ) . T @ inv ( H @ V @ H . T ) @ ( H @ beta - r ) PEP 492 , a.k.a native async syntax async def commit ( session , data ): ... async with session . transaction (): ... await session . update ( data ) ... PEP 498 , a.k.a f-strings >>> f 'The value is { value } .' 'The value is 80.' PEP 526 primes : List [ int ] = [] captain : str # Note: no initial value! class Starship : stats : ClassVar [ Dict [ str , int ]] = {} PEP 570 def add(x, y, /): return x + y PEP 572 , a.k.a walrus # Handle a matched regex if ( match := pattern . search ( data )) is not None : # Do something with match # A loop that can't be trivially rewritten using 2-arg iter() while chunk := file . read ( 8192 ): process ( chunk ) # Reuse a value that's expensive to compute [ y := f ( x ), y ** 2 , y ** 3 ] This huge list contains some of the (major~) changes that have been implemented since 3.5, there are also quite a few minor ones (like _ separator for numbers, 1_000_00 or PEP 530 for async comprehensions or even a new one for 3.9 to extend the decorator syntax, PEP 614 .) Well, since we all refreshed our memories, let's try to imagine a world where these features don't exist. Imagine not having access to f-strings, writing weird stuff to deal with your coroutines, or repeating yourself 2 times whenever you want to read chunks from a file, writing utility functions to merge 2 mappings and more awful cases. Except for maybe PEP 465 (I never needed it, probably because I don't do scientific programming. But from what I saw by looking at the examples in the PEP, it is quite good for people who work with data on a daily basis.) every feature in that list literally changed and actively affected how I write my Python code, right now. Seems like syntactical additions sounds great, why don't we add everything to the language? Let's start with adding regex literals, and then move forward to call pipeline operator, blah blah blah. If you are subscribed to the Python-ideas, then get ready to see tons of different, redundant new syntax proposals (even I, probably proposed a couple of very obscure and stupid ideas in the past). Though this brings me back to the point of, whether we should stop the syntactical growth at all or not. In the last 6 month, 3 different major syntax changes were proposed. I guess everyone is somewhat familiar with the pattern matching PEPs, PEP 622 (PEP 634, PEP 635, PEP 636). Also, there is PEP 637 for allowing keyword arguments on the subscript syntax [x=y, z=q] and PEP 638, for syntactic macros. I am not going to criticize any of these PEPs, but I'd like to ask you to think about them. Think of how they could affect you in 5 years. Think about whether your coding styles would change because of them, think about how good fit they are to the language, and most importantly think about whether you 'ACTUALLY' need them. Is there any 'ESSENTIAL' case that would be much better when you pass keyword arguments through slices instead of just making a call to some sort of get() function? Or can't we have syntactical customization without having an official definition of it? Does the syntax for 'patterns' (described by PEP 634) is a good fit for the Python language? There are a lot of blog posts, poll s and maybe hundreds of emails out there related to these proposals. I won't expect anyone to read them all, but if you want to get a general idea just check some of them out. Have to say that, I'm extremely overwhelmed by seeing this amount of change proposals every day, in various places. No one is forcing me to read them, though it is just a burden that I am intentionally or unintentionally taking to see what are people looking for in the language that I (myself) probably will be stuck with for the next decade. Trying to comprehend what people are aiming with making language so complex with growing the syntax more and more every day. Don't forget that adding syntax is much more serious than adding a functionality to the runtime. That syntax will be the face of the language, and you won't be able to alter it even a little bit. We all saw what happened when the syntax was changed in a backwards-incompatible manner, and no one wants to go through that again. This is why I am just asking you to think about whether do you believe these would be good fits for the language, whether they will worth to their imponderable cost. This is just me, throwing a bunch of questions into the void. If you want to talk more about these, feel free to send me an email (batuhan [at] python [dot] org) or reach me through twitter (open DM for all, @isidentical).","tags":"Languages","url":"https://tree.science/syntactical-growth-of-python.html","loc":"https://tree.science/syntactical-growth-of-python.html"},{"title":"What the Backtracking???","text":"Who loves restrictions? Even they might provide some kind of speed-ups or ensures the simplicity of algorithms. I guess the answer will be, most of the 'reasonable' people. But I can't say that I am one of them when I want to get some fun so I'll go forward and try to bring backtracking to a 3rd party simple push-down stateful LL(1) python parser. Disclaimer: this post is written with no actual intent of production-ready code, just an experiment about can we do or not as a fun project. What is 'Backtracking'? The simplest (but definitely not to most formal) way of I guess describing this concept is doing an analogy with database transitions and rollbacks. During a parse operation; when encountered with multiple transitions for a single token and the current state is suitable for moving forward on more than one transitions that the input produced, every alternative path is going to be represented as a transaction. When starting to process an alternative, we will save the current state and locals (like the input position, since we are going to eat tokens to determine if we are on the right path or not) and then if we fail, we'll rollback to the state we have saved. If we succeed, we'll move on. Multiple States from a 'Single Token' If you have worked with a language that the identifiers have the same form as other language structures, there is a concept of 'keywords'. Let's imagine the 'else' keyword, in theory, it suits the condition of a normal identifier and it would be possible for you to assign it, but for state machines that need to determine routes based on the tokens, instead of checking out the value of every different token, they use the value of only identifier tokens as different states on the transition table when it is reserved as a string on the grammar. def token_to_transition ( parser , token ): if token in parser . reserved_strings : return token . value else : return token . type But you might ask that there is still only one transition that is returned from that function even the input was a keyword, and that means there are no alternatives. You are right. With this kind of implementation, you can not use reserved strings (a.k.a keywords) on other occassions even where the type of token ( NAME ) suits. You can only use them at places where you specified as 'else' on the raw grammar. At least without 'soft keywords'. Soft Keywords Uhm, another different concept that got into our lives. I initially saw this one in a commit to CPython and seen the first real example of it when reading the PEP 622. It is a proposal about bringing a 'pattern matching statement' to the Python. Since introducing a new statement should be handled with a keyword, they choose to use 'match' and 'case'. But reserving another name during a new python version will break a lot of code, a lot. It will even break the use cases in the stdlib (e.g: re.match() ), of course, that authors considered that too and they made these 2 keywords 'soft'. The keywords act like normal keywords on the contexts and on other occasions they act like a usual 'NAME' token. For doing that when there is some kind of ambiguity, they try all alternatives and backtrack. Let's draft a token_to_transition function with supporting 'soft' keywords. Now the ' token_to_transition ' function will return either a 1-element tuple or a 2-element tuple depending on the code-path. def token_to_transition ( parser , token ): if token in parser . soft_keywords : return token . value , token . type elif token in parser . reserved_strings : return token . value , else : return token . type , So, if you make 'import' a soft keyword, then when the parser is in a state that expects either a 'NAME' or an 'import' keyword, it will try both and see which one of the path will be successfully continues. Let's Implement So, what we first need is determining some kind of strategy about whether if we are going to consume all tokens beforehand and fill the memory, or not. As you might remember from my previous post that, it is not uncommon for parsers to take lazy lexers as their producer and taking input one by one as they proceed. They also don't store the results from previously produced tokens. But when we're going to do backtracking, we need to consume 'enough' tokens beforehand and need to also rollback when we fail to proceed that alternative. Since, consuming all the tokens beforehand wouldn't be as pleasant as we used to in times before backtracking, I plan to operate in the best way I can. And it is going to be operating as usual when there are no ambiguity or place to backtrack, and when there is we are going to consume as little as we can and as soon as we are clear, we'll go back to the old way and output one by one. def parse(self, tokens): first_dfa = self._pgen_grammar.nonterminal_to_dfas[self._start_nonterminal][0] self.stack = Stack([StackNode(first_dfa)]) + self._tokens = _TokenGeneratorProxy(tokens) - for token in tokens: + for token in self._tokens: self._add_token(token) Let's try to draft a _TokenGeneratorProxy class; class _TokenGeneratorProxy : def __init__ ( self , generator ): self . _tokens = generator self . _counter = 0 def __iter__ ( self ): return self def __next__ ( self ): token = next ( self . _tokens ) self . _counter += 1 return token We just implemented an iterator protocol with __iter__ (which returns the iterator) and the __next__ (the function that is called on every iteration). For now, we're just keeping a counter and returning the value we get from the original producer. But what we need is a way of looking ahead without breaking the whole mechanism. What I have in mind is something like this; def process ( self , token ): ... self . eat_and_print_next_token () with self . _tokens . release () as proxy : for needed in range ( 2 ): proxy . eat ( needed ) print ( 'end of for loop' ) proxy . eat ( 0 ) self . eat_and_print_next_token () self . eat_and_print_next_token () self . eat_and_print_next_token () Which in theory should work like this; eating 'NAME' as the position '0' releasing the producer eating 'OPERATOR' as the position '0' eating 'NAME' as the position '1' end of for loop using cached token 'OPERATOR' from the position '0' locking the producer using cached token 'OPERATOR' from the position '1' using cached token 'NAME' from the position '2' eating 'NEWLINE' as the position '3' It might look a little bit complex, but if I have to explain I would say that, without any 'release' operation, the proxy is going to just consume from the producer and output without storing any kind of token. When there is a release action, it will lock a point (the counter), and record all the tokens that it will consume until it is locked again (exit of the context manager). When locked, the proxy.eat will eat in a relative range, so if you have eaten 1 token before (index 0) the lock operation, and try to perform proxy.eat(0) , it will eat the token in the position of index 1. Also, when you performed proxy.eat(0) after a proxy.eat(0) it will return the same thing (if performed with-in the same point). Also, it doesn't matter how far you have eaten in a lock, it will be reset to the point of the lock after the re-locking operation and for the ones you have already consumed, they will come up from the cache. @contextmanager def release ( self ): self . _release_ranges . append ([ self . _counter , None , []]) try : yield self finally : # Lock the last release range to the final position that # has been eaten. total_eaten = len ( self . _release_ranges [ - 1 ][ 2 ]) self . _release_ranges [ - 1 ][ 1 ] = self . _counter + total_eaten The _release_ranges is a list of 3-item length records. The first one points the state that we started eating tokens, the second one points the end (which is set after the re-locking operation), and the third one is another list that consists from the tokens that are eaten on that range. def eat ( self , point ): eaten_tokens = self . _release_ranges [ - 1 ][ 2 ] if point < len ( eaten_tokens ): return eaten_tokens [ point ] else : while point >= len ( eaten_tokens ): token = next ( self . _tokens ) eaten_tokens . append ( token ) return token The eaten_tokens (the last element of the record) is going to be ordered list so the index will point to the actual token that lays in self._counter + point . It will also work if you try to jump. def __next__ ( self ): # If the current position is already compromised (looked up) # return the eaten token, if not just go further on the given # token producer. for start , end , tokens in self . _release_ranges : assert end is not None if start <= self . _counter < end : token = tokens [ self . _counter - start ] break else : token = next ( self . _tokens ) self . _counter += 1 return token And finally, we need an extra for loop to go through all release records and try to match the current point and if found, just use it. If not, we'll allocate a new token. def can_advance ( self , to ): # Try to eat, fail if it can't. The eat operation is cached # so there wont be any additional cost of eating here try : self . eat ( to ) except StopIteration : return False else : return True Since we already caching the results of eat , it is going to be much easier to implement a can_advance functionality which will just try to go to the selected pointed, if the producer can produce no more, it will just return False . Otherwise will result with a True . Final Quest: Implementing the real backtracking Since we already created the way of eating / caching tokens from a lazy producer, we can safely go forward and modify the parser. As a short summary of the mechanism; > parse for token in tokens: add_token(token) > add_token transition = token_to_transition(token) while True: try: plan = stack[-1].dfa.transitions[transition] break except KeyError: if stack[-1].dfa.is_final: self._pop() else: self.error_recovery(token) return stack[-1].dfa = plan.next_dfa As you might be seen, it basically converts the input to a transition then finds the next state by checking out the transition table of the last state. When it founds the next transition, it sets it and continues. Let's continue step by step. The first thing is that, we changed the token_to_transition function to output multiple transitions, so we need to change there to. - transition = _token_to_transition(grammar, type_, value) + possible_transitions = _token_to_transition(grammar, type_, value) Also, it is going be too much for us to fit the functionality of finding the next state into a try/except , so we're just going to split out too. while True : try : plan = get_possible_plan ( possible_transitions , stack [ - 1 ] . dfa . transitions ) if plan is None : raise KeyError break except KeyError : ... # pop / error_recovery The get_possible_plan is going to return None , only if there is nowhere to proceed with the given transition. For just preserving the old habit of raising KeyError , we're just going to raise that KeyError (which was used to raised from accessing the transition table). Let's move forward with our precious get_possible_plan function which will eventually have some kind of backtracking. def get_possible_plan ( possible_transitions , transition_table ): possible_plans = [] for transition in possible_transitions : possible_plan = transition_table . get ( transition ) if possible_plan is not None : possible_plans . append ( possible_plan ) The function will get the possible_transitions , and a transition_table to check them against. It will create a temporary list that is going to contain all possible routes with given states. Then we'll go through all the states and try to find a place for them in the transition table. if len ( possible_plans ) == 0 : return None elif len ( possible_plans ) == 1 : return possible_plans [ 0 ] If there is nothing inside of possible_plans , we're just going to return None (which as I mentioned before, is going to equal of \"we are done here\"). If there is only one possible route, it is so nice, we can just return it and exit without any trouble. But, what if there are more then one possible plan with the current transitions. It is where, backtracking comes in. dead_plans = set () possible_plan_table = { possible_plan : [ possible_plan ] for possible_plan in possible_plans } We are going to keep 2 different tables. One is going to be a set of dead_plans , and the other one is going to be a mapping of the possible routes and how far they can proceed. Like if we have 2 possible routes and one can eat 3 token more and then fail, and if the other one can eat 4 and fail, we'll go for the latter. with self . _tokens . release () as token_proxy : counter = 0 while self . _superior_plan ( possible_plan_table ) is None : if not token_proxy . can_advance ( counter ): break # nothing to do, get the best plan we have token = token_proxy . eat ( counter ) next_transitions = _token_to_transition ( grammar , * token [: 2 ]) So we are always going to check out if there are any superior plans with self._superior_plan , and if we found that all plans except one is dead, we're just going to choose the one and stop consuming the token proxy and re-lock the current state. If we can't go further, we are just going to stop and return the first plan (which I don't expect will happen ever). for origin , possible_plans in possible_plan_table . items (): current_plan = possible_plans [ - 1 ] if origin in dead_plans : continue for dfa_push in reversed ( current_plan . dfa_pushes ): next_possible_plan = get_possible_plan ( next_transitions , dfa_push . transitions ) if next_possible_plan is not None : break else : dead_plans . add ( origin ) continue possible_plans . append ( next_possible_plan ) Here it comes the real backtracking. We are moving on all possible plans, and finding the current state which holds the transition table ( current_plan ). After that, we check if this plan is a dead-end or no. If it is, we are just going to pass the backtracking step and continue on the next plan. The for loop below that part is iterating over all the dfa pushes, which is actually the rules that it has entered. If we wanted to see what happens when we get import of the import x , the pushes are things like these; [ <DFAState: stmt is_final=True>, <DFAState: simple_stmt is_final=False>, <DFAState: small_stmt is_final=True>, <DFAState: import_stmt is_final=True>, <DFAState: import_name is_final=False> ] The reason we iterate a reversed version of it is finding a place where we can proceed. When we got a push that actually produces results with our current strategy, we choose to go with it and exit from the loop. If we can't find anything that actually works, we are just going to add this plan's root to the dead_plans set and won't ever try to progress on it. And finally, if we found either a supeior plan or consumed all the input (which I expect not to happen, unless some kind of invalid input) we're just going to exit with the supeior plan; return self . _superior_plan ( possible_plan_table , winner = True ) This was it, I really enjoyed when I was developing this since it sounded like a really funny thing to experiment with in the beginning. By the way if you have similar thoughts or new ideas about this subject, just let me know at twitter/@isidentical , I really would like to hear from you.","tags":"Parsers","url":"https://tree.science/what-the-backtracking.html","loc":"https://tree.science/what-the-backtracking.html"},{"title":"Lazy Lexers","text":"Generators are on-premise iterators that output when asked for, shortly they provide the ability of lazy evaluation. One great example of this shiny feature is lexers. A lexer is a program, a function that takes raw source code as input and outputs certain parts of it with identifying according to some pre-defined categories. Input: 'print(2+2)' ------------------------------------------- 1,0-1,5: NAME 'print' 1,5-1,6: OP '(' 1,6-1,7: NUMBER '2' 1,7-1,8: OP '+' 1,8-1,9: NUMBER '2' 1,9-1,10: OP ')' The example above shows tokens that belong to a single-word (no space between any character group) source code. The lexer has certain rules that explain what a token looks like. It might be a regex, or a handwritten way of searching characters. The example above starts with the p character, and sees it is a valid identifier start so continues to eat next characters until it reaches something that is not alphanumeric ( ( ). After that, it switches to the 'OP' rule, which has certain operator forms like ( or >= . When it gets a match on that, it switches again to a different rule and so on... Certain parsing methodologies requires to see all tokens before starting to parse, but that is not always the case. What if you have a very simple parser, which doesn't require to see upcoming tokens unless it can't parse the current state? It is where a lazy tokenizer comes in. Let's imagine a simple tokenizer which only parses digits and '+'/'-' operators. And there is also a syntactical verifier that ensures there is always an operator between 2 digits and it goes like that. When we input '2 + 2', it return True and when we input '2 2 +', or '2 + + 2' it raises an error. And if we give '$ 2 + 2', then it won't start, there is going to be an error raised from the tokenizer side. But what if we input something that has multiple defects, something like this, '2 + + 2 + 3 $ 4'. Where it should raise an error? As I said before, it depends, but for a simple parser, you want to show your user the errors gradually so they solve them one by one. So we should first give them the ParsingError and then after they handled it we should raise the TokenizationError so they fix it too, but incrementally. And there is no point in keeping '3' in the memory when there is a problem that happens before. Let's draft an example in Python. It is going to be very simple. def lexer ( source ): for word in source . split (): print ( f 'Lexing new word: { word } ' ) if word . isdigit (): yield Tokens . DIGIT elif word in { \"+\" , \"-\" }: yield Tokens . OPERATOR else : raise LexerError ( f \"Unknown token: ' { word } '\" ) We will go through the words (space splitted) and then if we encounter a digit or an operator, we return its type. If not, we will raise a LexerError . >>> producer = lexer('2 + + 2 + 3 $ 4') >>> next(producer) Lexing new word: 2 <Tokens.DIGIT: 1> >>> next(producer) Lexing new word: + <Tokens.OPERATOR: 2> >>> next(producer) Lexing new word: + <Tokens.OPERATOR: 2> >>> next(producer) Lexing new word: 2 <Tokens.DIGIT: 1> >>> next(producer) Lexing new word: + <Tokens.OPERATOR: 2> >>> next(producer) Lexing new word: 3 <Tokens.DIGIT: 1> >>> next(producer) Lexing new word: $ __main__.LexerError: Unknown token: $ Let's draft about the verifier, which will use this producer; def verifier ( producer ): expected_type = Tokens . DIGIT for token_type in producer : if token_type is not expected_type : raise ParserError ( f 'Expecting { expected_type } but got { token_type } ' ) if expected_type is Tokens . DIGIT : expected_type = Tokens . OPERATOR elif expected_type is Tokens . OPERATOR : expected_type = Tokens . DIGIT else : raise UnexpectedStateError else : return True Since we have only one way to go, we can use a single state-d system. We have 'expected_type' which points to the next thing that we need to get if we want to be syntactically correct. Our starting state will be a 'DIGIT', and then we will go through the producer and try to match the eaten tokens with our single state ('expected_type'). And after every the operation, we decide the next expected state by looking at the previous state. Let's give it a try: >>> verify('2 + 2') Lexing new word: 2 Lexing new word: + Lexing new word: 2 True >>> verify('2 + + 2 + 3 $') Lexing new word: 2 Lexing new word: + Lexing new word: + __main__.ParserError: Expecting Tokens.DIGIT but got Tokens.OPERATOR As you can see, it worked as we imagined. It didn't try to lex '$' sign because the parser failed before asking it to eat that character. It didn't even try to lex '3' so it early-failed and saved up some time. And if we try to fix this error; >>> verify('2 + 2 + 3 $') Lexing new word: 2 Lexing new word: + Lexing new word: 2 Lexing new word: + Lexing new word: 3 Lexing new word: $ __main__.LexerError: Unknown token: $ We get the lexer error. This was it. Thanks for reading, I know this was a relatively short blog post but I needed to explain why token producers are generally expressed as generators and what are the advantages (a hint about the new blog post coming is bringining 'backtracking').","tags":"Tokenizers","url":"https://tree.science/lazy-lexers.html","loc":"https://tree.science/lazy-lexers.html"},{"title":"Using Ancestral Chain in AST","text":"Tree-like data is a common consumption target of applications that are empowered by visitor pattern. A couple of use cases of this pattern includes code generators (a.k.a compilers), static code analysis tools (linters), and code formatters. Code generators take advantage of working with heterogeneous data. On the other side, linters make use of working with-in a state to process previously learned information about that codebase. class Emitter ( Visitor ): def visit_Module ( self , node ): with self . create_module () as module : self . generic_visit ( node ) return module def visit_BinOp ( self , node ): self . visit ( node . left ) self . visit ( node . right ) opcode = type ( node . op ) . __name__ . upper () self . module . emit ( Instruction ( f \"OPERATION_ { opcode } \" )) def visit_Constant ( self , node ): self . module . emit ( Instruction ( \"LITERAL\" , node . value )) As seen in the upper example, implementing a simple AST to some imaginary bytecode compiler isn't hard at all. It only requires a visitor class and implementation of how an individual node should act when seen. This kind of Visitor s actually take the whole responsibility of visiting the tree, in a way that can be used recursively. What I mean by \"recursive\" is that in common languages, fields on certain nodes contain heterogeneous data (sums). For example, rhs or lhs of a binary operation can be a Name , a Call , or simply another binary operation. Instead of having certain conditions for all of them, you can simply pass it to the Visitor and it will do the job of finding the right place to emit code for that node. int visit_constant ( Generator * gen , Node * node ) { return EMIT ( \"LITERAL\" , node -> v . Constant . value ); } int visit_binop ( Generator * gen , Node * node ) { CHECK ( visit_expression ( node -> v . BinOp . left )); CHECK ( visit_expression ( node -> v . BinOp . right )); return EMIT ( _operator_name ( node -> v . BinOp . operator )); } int visit_expression ( Generator * gen , Node * node ) { switch ( node -> kind ) { case Name_kind : return visit_name ( gen , node ); case BinOp_kind : return visit_binop ( gen , node ); case Constant_kind : return visit_constant ( gen , node ); } } In the example above, the visit_binop function doesn't need to know what is the type of lhs / rhs. It just knows it is an expression and passes it to a top-level visit_expression function which then handles the rest. Okay, we are good up to know, but what about some context dependant actions. Context Dependent Actions So, imagine you have a language that has certain features, but using two different esoteric functionality together should be forbidden. Grammars are not suitable for such actions because while the construction, no one knows what context you are in or who are your neighbors (you can actually record about it, but it would be a big mess). Since you are already using the visitor pattern to compile the AST, you can think that while compiling one of that features, you can peek around and find if it is good to use or not. For this example, I'll forbid using control flow elements inside of a finally . Let's see what we should handle For ( target = Name ( id = 'x' , ctx = Store ()), iter = Name ( id = 'y' , ctx = Load ()), body = [ Try ( body = [ Pass ()], handlers = [], orelse = [], finalbody = [ Continue ()], # <======= ), ], orelse = [], type_comment = None , ) What we are looking for a Continue (or Break() etc.) inside of a finally inside of a for loop. Let's try to draft an implemention of it. import ast class Compiler ( ast . NodeVisitor ): def visit_For ( self , node ): self . inside_for = True jumper = self . emit ( \"FOR_LOOP\" , node . target , node . iter ) self . emit_body ( node . body , bind = jumper ) self . inside_for = False Okay, from the initial point, we got ourselves a \"state\" ( self.inside_for ), it is no big deal. The only downside we can think of, for now, is the code looks kinda messy. We can probably write a context manager to set an instance variable, but I don't think that would be a pythonic idea. def visit_Try ( self , node ): with self . _try_guard ( node ) as guard : [ ... ] self . inside_finally = True [ ... ] self . inside_finally = False def visit_Continue ( self , node ): if self . inside_for and self . inside_finally : raise ValueError It started to have more states ( self.inside_finally ), more and more. Guess we can have a context manager to at least reduce this hand managing burden, but it won't solve the problem of thousands of different states that you may require or not. While the codebase grows, some of them will become obsolete, useless; and no one might just notice it. And the actual problem is that, this check is wrong, for cases like the code below. for x in y : try : pass finally : for y in z : continue This should successfully run because the continue belongs a different for loop, and that has nothing to do with the upper try/finally clause. We can probably solve this with more and more states but I won't advise it because it would make the code more confusing, complex and awful. As the title may suggest, the solution to this problem might be, the ancestral chains. It is some kind of backtracking to the original tree node from the child. >>> tree = ast.parse(\"2+2\", mode=\"eval\") >>> lhs = tree.body.left >>> lhs <ast.Constant object at 0x7f39270f9af0> >>> lhs.parent <ast.BinOp object at 0x7f39270f97d0> >>> lhs.parent.parent <ast.Expression object at 0x7f39270f9730> Having such a chain attached to node objects would allow us to search in that context and check if we have the right conditions to raise an error. Implementation Let's implement a simple relate(tree: AST) -> None function that would take the tree, and parent field to all nodes by traversing it. def relate ( tree ): tree . parent = None The first thing is setting the parent of root node. It is important because it will signal consumers to where they should stop. for parent in ast . walk ( tree ): ast.walk will walk in every node and leaf, which they all be parents (leaves might not, but that won't matter since the function below doesn't return anything on that case) for child in ast . iter_child_nodes ( parent ): child . parent = parent iter_child_nodes will yield all the first-level child nodes that belong to the given node. And then we can safely set the parent. It might be a cool idea to use weakref.ref here, but I don't think the parents will go away while the children are still living. Also, it might be good to have a flatten function that will iterate all parents , something like this def get_parents ( node ): parent = node while parent := parent . parent : yield parent This way we can do some kind of search among the parents >>> tuple ( get_parents ( lhs )) ( < ast . BinOp object at 0x7f46b7e3d190 > , < ast . Expression object at 0x7f46b7ed6c80 > ) >>> ast . Expression in tuple ( map ( type , get_parents ( lhs ))) True This was it, all we need is that searching for parents. If we want to repeat the Compiler example with the same the bug we had, it would be as simple and as pythonic as this; class AncestorChain ( set ): def __init__ ( self , * args ): return super () . __init__ ( args ) def __contains__ ( self ): return self . issubset ( map ( type , get_parents ( node ))) class Compiler ( ast . NodeVisitor ): [ ... ] FORBIDDEN_SET = AncestorChain ( ast . For , ast . Try ) def visit_Continue ( self , node ): if node in FORIBDDEN_SET : raise ValueError It is this simple, no states at all. Although we made the code simple, we want to also solve the bug. One thing that came to my mind for such cases is a utility function that returns the first occurrence of a type in that chain. This will help us to find the first occurrence of the try clause and then we can check its parent for the given for. def first_occurrence ( node , * ancestors ): for parent in get_parents ( node ): if type ( parent ) in ancestors : return parent else : return False class Compiler ( ast . NodeVisitor ): [ ... ] def visit_Continue ( self , node ): if node in FORIBDDEN_SET : for_clause = first_occurrence ( node , ast . For ) try_clause = first_occurrence ( node , ast . Try ) if is_parented_by ( try_clause , for_clause ): raise ValueError What it does is that it finds the first try/finally and the first for clause and then it checks that if that continue 's bound for is also the parent of that try/finally clause, if so, it raises the value error, but cases like this; for item in items : try : pass finally : for child in children : continue it will be silenced. So, instead of having millions of states, we solved the problem with 3 lines of extra code to the compiler. Having an advantage of working in a dynamic language, will certainly helped the simpler implementation of this ancestral chains, but I dont think there will be a decent difference on languages like C about the implementation's look. And by the way, thanks for reading. If you want to contact to me, please check out the social section in the footer, or mail me directly from isidentical [at] tree.science . Appendix 1: Full Code import ast def relate ( tree ): tree . parent = None for parent in ast . walk ( tree ): for child in ast . iter_child_nodes ( parent ): child . parent = parent def get_parents ( node ): parent = node while parent := parent . parent : yield parent class AncestorChain ( set ): def __init__ ( self , * args ): return super () . __init__ ( args ) def __contains__ ( self , node ): return self . issubset ( map ( type , get_parents ( node ))) def first_occurrence ( node , * ancestors ): for parent in get_parents ( node ): if type ( parent ) in ancestors : return parent else : return False def is_parented_by ( node , * ancestors ): for parent in get_parents ( node ): if parent in ancestors : return True else : return False","tags":"Abstract Syntax Trees","url":"https://tree.science/using-ancestral-chains-in-ast.html","loc":"https://tree.science/using-ancestral-chains-in-ast.html"},{"title":"A few thoughts on the Zephyr's ASDL","text":"The Zephyr Abstract Syntax Description Language or shortly Zephyr's ASDL is a well known; mature (released at 1997~), descriptive language for defining ASTs (nodes and leaves) and other tree-like data structures. When it got released (and even after one or two decades) it was more than capable of being used in major compilers, including CPython's bytecode compiler (from v2.5~). But after having quite a bit of experience with it, I started to realize that it can be improved in a few ways. The whole post will contain my own humble opinions. Please feel free share your own ones with me through the contact information at the end of the post. I'm currently working on a demo project to enpower all the ideas that are being mentioned below, and it would be really nice to hear from you about these. Type System Every field declaration in ASDL consists from this form [type][qualifier]? [name] ; where [type] is either something defined in the current spec, or a built-in one (such as identifier ). And the [qualifier] is a mutually exclusive and optional qualifier for the given type. There are 2 kinds of qualifiers [qualifier]={?, *} . A question mark ( ? ) means it can be either empty or something that belongs to the [type] . On the other side, a star ( * ) means it is a zero or more element sequence of given [type] . In theory, these 2 qualifiers might seem enough, but giving a basic example might prove the otherwise. Let's imagine a simple AST of a python function. function = Function(identifier name, expr? returns, stmt* body) It has a name, an optional return annotation, and a list of statements as it's body. Which looks very accurate, right? def foo () -> int : pass def bar (): pass pass If we try to address these functions in the AST which we created earlier, it will look something like this; Function ( \"foo\" , Expr ( int ), [ PassStmt ()]) Function ( \"bar\" , None , [ PassStmt (), PassStmt ()]) And there is nothing that looks wrong with this form, as long as it is generated by some kind of parser. But Python allows users to craft and compile arbitrary ASTs. As you might know, the function bodies in Python have to at least 1 statement, but the ASDL implies that it might have zero (since * means zero or more). There goes the conflict, this AST, Function(\"baz\", None, []) is valid according to the ASDL spec, but it later on it might crash the interpreter or might not pass the validation at all. For CPython, there is a custom AST validator, which comes with the burden of maintenance, just for ensuring that user crafted AST's won't crash the compiler. >>> import ast >>> foo_mod = ast . parse ( \"def foo(): pass\" ) >>> foo_mod . body [ 0 ] . body . clear () >>> compile ( foo_mod , \"<test>\" , \"exec\" ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > ValueError : empty body on FunctionDef <= custom error static int validate_nonempty_seq ( asdl_seq * seq , const char * what , const char * owner ) { if ( asdl_seq_LEN ( seq )) return 1 ; PyErr_Format ( PyExc_ValueError , \"empty %s on %s\" , what , owner ); return 0 ; } static int validate_body ( asdl_seq * body , const char * owner ) { return validate_nonempty_seq ( body , \"body\" , owner ); } ... case FunctionDef_kind : return validate_body ( stmt -> v . FunctionDef . body , \"FunctionDef\" ) Also this is not the only case where the AST doesn't comply with ASDL. For an example, the AST of an dictionary defined as Dict(expr* keys, expr* values) , which means that it has two list of expressions that are named keys and values . That makes sense since, AST of {'a': 'b'} is just Dict([Constant('a')], [Constant('b')]) . But when it comes to dict unpacking inside of another dictionary with double-star operator, the AST looks like this; Input: {**a, b:c} Output: Dict( keys=[ None, Name(id='b', ctx=Load()), ], values=[ Name(id='a', ctx=Load()), Name(id='c', ctx=Load()), ], ) Did you see that there is an outlier among the keys , the None . This is because that field qualifiers are mutually exclusive, and you can't chain them. Things like this would make the ASDL a context-dependent thing and in some cases, they might increase the maintenance burden (such as external verifiers, which I'll also address in the next section). The solution would be as simple as just extending the current qualifiers and make them chainable. There are 2 design I have in my mind. The first one is introducing new qualifiers in the same form * => zero or more sequence + => one or more sequence ? => optional [<field type>] => also optional but chainable FunctionDef(identifier name, expr? returns, stmt+ body) Dict([expr]* keys, expr* values) the second one is kind-a different might be hard to process in big ASDL's but more explicit. ZeroOrMore[<field type>] => such as * OneOrMore[<field type>] => such as + Opt/Optional[<field type>] => such as * or [<field type>] FunctionDef(identifier name, Opt[returns], OneOrMore[body]) Dict(ZeroOrMore[Opt[keys]], ZerOrMore[values]) ASDL actions Integrating some source code inside of grammars isn't a new idea, a recent example would be the Python's new parser generator, and the grammar it consumes. I believe that this can be integrated very quickly to the ASDL itself, with a new but not an unorthodox syntax. The purpose of these actions is going to be both verification and transitions (not limited to that). It might open a way to language extensions. Bringing such actions would require a metadata format to the ASDL modules, the best form I can think of is something similar to python decorators that will annotate the ASDL modules (namespaces, which are not part of the original paper ). @<key> <value> module <name> {} @actions C @version 3.8 module Example {} The action syntax will depend on the action's purpose {verify, transition} $left::right [where $condition] { [ACTION] } Verifier Actions As I mentioned earlier, languages that allow users to create external ASTs requires a custom validation step. Type checking will help in most cases, but there will be still some esoteric ones left. It might be a controversial thing since some people might not want to host their source code inside of a text spec (I dont know, maybe for their linters / formatters, or other purposes), but this will ensure that the validation process is public and the clients of this AST will know what nodes will be validated and which kind of methods will be used for their validation. verify $nodes::$fields [where $condition] { [ACTION] } verify Dict::(keys, values) { return len(keys) == len(values) } verify ImportFrom::level { return level >= 0 } verify Try::(handlers, finalbody, orelse) where len(handlers) == 0 { if len(finalbody) == 0 and len(orelse) > 0: return False ... } Transitioning Here it comes the other big problem and a use case for actions, the AST changes. If you are writing some kind of tool that consumes the AST (e.g: linter), it is not uncommon for you to get broken in every release. The reason for that is AST is also an internal format and things might just change for internal reasons and no one gives you a guarantee about it won't change again. So you have to test your tool on every major release and ensure the breakages are gone by creating tons of workarounds. This is the case for even the simplest change, like changing the name of a node. The solution would be a simple layer of \"compatibility\". The way it should work is that, for old nodes, it is going to keep the same structure as the old ASTs even though the name of the form of that node is changed. Achieving such a thing would be available in 2 ways: keeping ASDL of every version (and it would be definitely a mess), or only the generated code for that nodes as a part of that \"compatibility\" layer. I'd personally go for the latter. Let's do an imaginary example of 3 different language versions; @version 3.6 module Example { number = NumOrFloat(object value) | Complex(object value) } @version 3.7 module Example { number = Num(object value) | Float(object value) | Complex(object value) @version 3.8 module Example { number = Number(object value, str kind) The 3.6 version has 2 nodes, NumOrFloat for numbers and floats and Complex for imaginary numbers. The 3.7 form splits NumOrFloat into 2 different nodes, a Num node and a Float node. And finally, the 3.8 version has only 1 constructor, it is the Number , with an additional kind field. In theory that no matter which version you are, in the \"compatibility\" layer, you would only have the NumOrFloat and the Complex nodes. For providing that we need some kind of action to do the transition. transition $source::$destination [where $condition] { [ACTION] } transition Number::(Num, Float, Complex) { switch (origin->kind) { case 'integer': return Num(origin->value); case 'float': return Float(origin->value); case 'complex': return Complex(origin->value); } } The example upper takes a 3.8 Number node and outputs a 3.7 node ( Num , Float , or Complex ). This is also going to be the case for 3.7 , it will take a 3.7 AST and output a 3.6 version. So in the end, all ASTs will be the same in the imaginary \"compatibility\" layer. @version 3.7 module Example { number = Num(object value) | Float(object value) | Complex(object value) transition Float::NumOrFloat { return NumOrFloat(origin->value); } } Thanks I guess this is all, thanks for reading this and if you have any extra thoughts about this, I really want to listen to all of them. Please contact me through isidentical [at] tree.science or twitter/telegram/discord ( @isidentical )","tags":"Abstract Syntax Trees","url":"https://tree.science/transitional-asdl.html","loc":"https://tree.science/transitional-asdl.html"}]};